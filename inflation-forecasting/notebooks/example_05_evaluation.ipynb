{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d5e3a8",
   "metadata": {},
   "source": [
    "# 05. Model Evaluation and Comparison\n",
    "\n",
    "**Objective:** Compare all forecasting models and generate comprehensive evaluation results\n",
    "\n",
    "**Models Compared:**\n",
    "- **Econometric:** Random Walk, VAR, ARIMA\n",
    "- **Linear ML:** LASSO, Ridge, Elastic Net, LARS\n",
    "- **Nonlinear ML:** Random Forest, SVR, XGBoost\n",
    "\n",
    "**Evaluation:**\n",
    "- Performance metrics (RMSFE, MAPE)\n",
    "- Visual comparisons\n",
    "- Statistical tests\n",
    "- Export results for LaTeX report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51aea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom metrics\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.metrics import rmsfe, mape\n",
    "\n",
    "# Paths\n",
    "RESULTS_PATH = Path('../results')\n",
    "TABLES_PATH = RESULTS_PATH / 'tables'\n",
    "FIGURES_PATH = RESULTS_PATH / 'figures'\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db41325",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from each model category\n",
    "econometric_results = pd.read_csv(TABLES_PATH / 'econometric_results.csv')\n",
    "linear_ml_results = pd.read_csv(TABLES_PATH / 'linear_ml_results.csv')\n",
    "nonlinear_ml_results = pd.read_csv(TABLES_PATH / 'nonlinear_ml_results.csv')\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([\n",
    "    econometric_results,\n",
    "    linear_ml_results,\n",
    "    nonlinear_ml_results\n",
    "], ignore_index=True)\n",
    "\n",
    "# Add model category\n",
    "all_results['category'] = [\n",
    "    'Econometric' if model in ['Random Walk', 'ARIMA', 'VAR'] else\n",
    "    'Linear ML' if model in ['LASSO', 'Ridge', 'Elastic Net', 'LARS'] else\n",
    "    'Nonlinear ML'\n",
    "    for model in all_results['model']\n",
    "]\n",
    "\n",
    "print(\"All Models Performance:\")\n",
    "print(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d919a93",
   "metadata": {},
   "source": [
    "## 2. Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc58356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by RMSFE\n",
    "all_results_sorted = all_results.sort_values('RMSFE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE MODEL PERFORMANCE RANKING\")\n",
    "print(\"=\"*70)\n",
    "print(all_results_sorted.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save complete results\n",
    "all_results_sorted.to_csv(TABLES_PATH / 'all_models_results.csv', index=False)\n",
    "print(\"\\n✓ Complete results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe85790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best models in each category\n",
    "best_by_category = all_results.loc[all_results.groupby('category')['RMSFE'].idxmin()]\n",
    "\n",
    "print(\"\\nBest Model in Each Category:\")\n",
    "print(best_by_category[['category', 'model', 'RMSFE', 'MAPE']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4874a4",
   "metadata": {},
   "source": [
    "## 3. Visualization - Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot - RMSFE comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RMSFE\n",
    "colors = ['#1f77b4' if cat == 'Econometric' else '#ff7f0e' if cat == 'Linear ML' else '#2ca02c' \n",
    "          for cat in all_results_sorted['category']]\n",
    "ax1.barh(all_results_sorted['model'], all_results_sorted['RMSFE'], color=colors)\n",
    "ax1.set_xlabel('RMSFE (Lower is Better)', fontsize=12)\n",
    "ax1.set_title('Model Comparison - RMSFE', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# MAPE\n",
    "ax2.barh(all_results_sorted['model'], all_results_sorted['MAPE'], color=colors)\n",
    "ax2.set_xlabel('MAPE (%) (Lower is Better)', fontsize=12)\n",
    "ax2.set_title('Model Comparison - MAPE', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#1f77b4', label='Econometric'),\n",
    "    Patch(facecolor='#ff7f0e', label='Linear ML'),\n",
    "    Patch(facecolor='#2ca02c', label='Nonlinear ML')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=3, fontsize=11)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(FIGURES_PATH / 'all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64177031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category comparison - boxplot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# RMSFE by category\n",
    "all_results.boxplot(column='RMSFE', by='category', ax=ax1)\n",
    "ax1.set_title('RMSFE by Model Category')\n",
    "ax1.set_xlabel('Model Category')\n",
    "ax1.set_ylabel('RMSFE')\n",
    "plt.sca(ax1)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# MAPE by category\n",
    "all_results.boxplot(column='MAPE', by='category', ax=ax2)\n",
    "ax2.set_title('MAPE by Model Category')\n",
    "ax2.set_xlabel('Model Category')\n",
    "ax2.set_ylabel('MAPE (%)')\n",
    "plt.sca(ax2)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'category_comparison_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d85ad",
   "metadata": {},
   "source": [
    "## 4. Generate LaTeX Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99912db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table for paper\n",
    "def generate_latex_table(df, caption=\"Model Performance Comparison\", label=\"tab:model_comparison\"):\n",
    "    \"\"\"\n",
    "    Generate a LaTeX table from results dataframe\n",
    "    \"\"\"\n",
    "    latex_str = \"\\\\begin{table}[htbp]\\n\"\n",
    "    latex_str += \"  \\\\centering\\n\"\n",
    "    latex_str += f\"  \\\\caption{{{caption}}}\\n\"\n",
    "    latex_str += f\"  \\\\label{{{label}}}\\n\"\n",
    "    latex_str += \"  \\\\begin{tabular}{llcc}\\n\"\n",
    "    latex_str += \"    \\\\hline\\n\"\n",
    "    latex_str += \"    \\\\textbf{Category} & \\\\textbf{Model} & \\\\textbf{RMSFE} & \\\\textbf{MAPE (\\\\%)} \\\\\\\\\\n\"\n",
    "    latex_str += \"    \\\\hline\\n\"\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        latex_str += f\"    {row['category']} & {row['model']} & {row['RMSFE']:.4f} & {row['MAPE']:.2f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_str += \"    \\\\hline\\n\"\n",
    "    latex_str += \"  \\\\end{tabular}\\n\"\n",
    "    latex_str += \"\\\\end{table}\\n\"\n",
    "    \n",
    "    return latex_str\n",
    "\n",
    "# Generate table\n",
    "latex_table = generate_latex_table(all_results_sorted)\n",
    "\n",
    "# Save to file\n",
    "with open(TABLES_PATH / 'model_comparison_latex.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX Table Generated:\")\n",
    "print(\"=\"*70)\n",
    "print(latex_table)\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✓ LaTeX table saved to results/tables/model_comparison_latex.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb083b",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by category\n",
    "category_summary = all_results.groupby('category').agg({\n",
    "    'RMSFE': ['mean', 'std', 'min', 'max'],\n",
    "    'MAPE': ['mean', 'std', 'min', 'max']\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nPerformance Summary by Category:\")\n",
    "print(\"=\"*70)\n",
    "print(category_summary)\n",
    "print(\"=\"*70)\n",
    "\n",
    "category_summary.to_csv(TABLES_PATH / 'category_summary.csv')\n",
    "print(\"\\n✓ Category summary saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall best model\n",
    "best_model = all_results_sorted.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL BEST MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {best_model['model']}\")\n",
    "print(f\"Category: {best_model['category']}\")\n",
    "print(f\"RMSFE: {best_model['RMSFE']:.4f}\")\n",
    "print(f\"MAPE: {best_model['MAPE']:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9e672",
   "metadata": {},
   "source": [
    "## 6. Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "insights = []\n",
    "\n",
    "# Best model\n",
    "insights.append(f\"1. Best Overall Model: {best_model['model']} (RMSFE: {best_model['RMSFE']:.4f})\")\n",
    "\n",
    "# Best in each category\n",
    "for _, row in best_by_category.iterrows():\n",
    "    insights.append(f\"   - Best {row['category']}: {row['model']} (RMSFE: {row['RMSFE']:.4f})\")\n",
    "\n",
    "# Category performance\n",
    "avg_rmsfe = all_results.groupby('category')['RMSFE'].mean().sort_values()\n",
    "insights.append(f\"\\n2. Category Performance (avg RMSFE):\")\n",
    "for cat, rmsfe in avg_rmsfe.items():\n",
    "    insights.append(f\"   - {cat}: {rmsfe:.4f}\")\n",
    "\n",
    "# Model count\n",
    "insights.append(f\"\\n3. Total models evaluated: {len(all_results)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save insights\n",
    "with open(RESULTS_PATH / 'key_insights.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(insights))\n",
    "\n",
    "print(\"\\n✓ Key insights saved to results/key_insights.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5d136",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Evaluation completed:**\n",
    "- ✓ All models compared\n",
    "- ✓ Performance metrics calculated\n",
    "- ✓ Visualizations created\n",
    "- ✓ LaTeX table generated for paper\n",
    "- ✓ Key insights identified\n",
    "\n",
    "**Files generated:**\n",
    "- `results/tables/all_models_results.csv`\n",
    "- `results/tables/category_summary.csv`\n",
    "- `results/tables/model_comparison_latex.tex`\n",
    "- `results/figures/all_models_comparison.png`\n",
    "- `results/figures/category_comparison_boxplot.png`\n",
    "- `results/key_insights.txt`\n",
    "\n",
    "**Next steps:**\n",
    "- Use these results to write your paper\n",
    "- Include the LaTeX table in your report\n",
    "- Analyze the best-performing models further\n",
    "- Consider ensemble methods combining top models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
